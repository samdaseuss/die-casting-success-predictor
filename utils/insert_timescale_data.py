import logging
from sqlalchemy import create_engine, text
import os
import json
import pandas as pd
from datetime import datetime, timedelta
import random
from typing import Dict, List, Optional
from pathlib import Path

logger = logging.getLogger(__name__)

# TimescaleDB ì—°ê²° ì„¤ì •
DB_CONFIG = {
    'host': os.getenv('POSTGRES_HOST', 'localhost'),
    'port': os.getenv('POSTGRES_PORT', '5434'),
    'database': os.getenv('POSTGRES_DB', 'diecasting_db'),
    'user': os.getenv('POSTGRES_USER', 'postgres'),
    'password': os.getenv('POSTGRES_PASSWORD', 'password')
}

def get_db_engine():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ ìƒì„±"""
    try:
        connection_string = f"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}"
        engine = create_engine(connection_string, pool_pre_ping=True)
        return engine
    except Exception as e:
        logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

def init_sensor_data_table():
    """sensor_data í…Œì´ë¸” ìƒì„± ë° ì´ˆê¸°í™”"""
    engine = get_db_engine()
    if not engine:
        return False
    
    try:
        with engine.connect() as conn:
            # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ (í•„ìš”í•œ ê²½ìš°)
            # conn.execute(text("DROP TABLE IF EXISTS sensor_data CASCADE;"))
            
            # ì„¼ì„œ ë°ì´í„° í…Œì´ë¸” ìƒì„±
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS sensor_data (
                    time TIMESTAMPTZ NOT NULL,
                    id BIGINT,
                    line TEXT,
                    mold_name TEXT,
                    working INTEGER,
                    molten_temp REAL,
                    facility_operation_cycletime INTEGER,
                    production_cycletime INTEGER,
                    low_section_speed REAL,
                    high_section_speed REAL,
                    cast_pressure REAL,
                    biscuit_thickness REAL,
                    upper_mold_temp1 REAL,
                    upper_mold_temp2 REAL,
                    lower_mold_temp1 REAL,
                    lower_mold_temp2 REAL,
                    sleeve_temperature REAL,
                    physical_strength REAL,
                    coolant_temperature REAL,
                    ems_operation_time INTEGER,
                    mold_code INTEGER,
                    passorfail TEXT,
                    prediction_confidence REAL DEFAULT 0.0,
                    data_hash TEXT,
                    source TEXT DEFAULT 'migration'
                );
            """))
            
            # í•˜ì´í¼í…Œì´ë¸”ë¡œ ë³€í™˜ (TimescaleDB)
            try:
                conn.execute(text("""
                    SELECT create_hypertable('sensor_data', 'time', 
                                            if_not_exists => TRUE);
                """))
                logger.info("TimescaleDB í•˜ì´í¼í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
            except Exception as e:
                if "already exists" in str(e).lower():
                    logger.info("í•˜ì´í¼í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
                else:
                    logger.warning(f"í•˜ì´í¼í…Œì´ë¸” ìƒì„± ê±´ë„ˆëœ€: {e}")
            
            # ì¸ë±ìŠ¤ ìƒì„±
            conn.execute(text("""
                CREATE INDEX IF NOT EXISTS idx_sensor_data_passorfail 
                ON sensor_data (passorfail, time DESC);
            """))
            
            conn.execute(text("""
                CREATE INDEX IF NOT EXISTS idx_sensor_data_mold_code 
                ON sensor_data (mold_code, time DESC);
            """))
            
            conn.commit()
            logger.info("TimescaleDB í…Œì´ë¸” ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
    except Exception as e:
        logger.error(f"í…Œì´ë¸” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

def generate_sample_data(count: int = 100, start_time: Optional[datetime] = None) -> List[Dict]:
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    if start_time is None:
        start_time = datetime.now() - timedelta(hours=24)
    
    sample_data = []
    
    for i in range(count):
        # ì‹œê°„ì„ ìˆœì°¨ì ìœ¼ë¡œ ì¦ê°€
        timestamp = start_time + timedelta(minutes=i * 5)
        
        # ëœë¤í•˜ê²Œ Pass/Fail ê²°ì • (90% Pass, 10% Fail)
        pass_or_fail = "Pass" if random.random() > 0.1 else "Fail"
        
        # Failì¸ ê²½ìš° ì¼ë¶€ ê°’ë“¤ì„ ì´ìƒí•˜ê²Œ ì„¤ì •
        if pass_or_fail == "Fail":
            molten_temp = random.uniform(580, 620)  # ì •ìƒ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨
            cast_pressure = random.uniform(45, 60)   # ë†’ì€ ì••ë ¥
            upper_temp1 = random.uniform(180, 220)   # ë†’ì€ ì˜¨ë„
        else:
            molten_temp = random.uniform(650, 680)   # ì •ìƒ ë²”ìœ„
            cast_pressure = random.uniform(30, 45)   # ì •ìƒ ì••ë ¥
            upper_temp1 = random.uniform(150, 180)   # ì •ìƒ ì˜¨ë„
        
        data = {
            'time': timestamp.isoformat(),
            'id': 73612 + i,
            'line': f"Line_{random.randint(1, 3)}",
            'mold_name': f"Mold_{random.choice(['A', 'B', 'C'])}_{random.randint(1, 5)}",
            'working': random.randint(0, 1),
            'molten_temp': round(molten_temp, 2),
            'facility_operation_cycletime': random.randint(25, 35),
            'production_cycletime': random.randint(28, 40),
            'low_section_speed': round(random.uniform(0.1, 0.3), 3),
            'high_section_speed': round(random.uniform(1.5, 3.0), 3),
            'cast_pressure': round(cast_pressure, 2),
            'biscuit_thickness': round(random.uniform(8, 12), 2),
            'upper_mold_temp1': round(upper_temp1, 2),
            'upper_mold_temp2': round(upper_temp1 + random.uniform(-5, 5), 2),
            'lower_mold_temp1': round(random.uniform(140, 170), 2),
            'lower_mold_temp2': round(random.uniform(140, 170), 2),
            'sleeve_temperature': round(random.uniform(200, 250), 2),
            'physical_strength': round(random.uniform(200, 300), 2),
            'coolant_temperature': round(random.uniform(25, 35), 2),
            'ems_operation_time': random.randint(5, 15),
            'mold_code': random.randint(1001, 1010),
            'passorfail': pass_or_fail,
            'prediction_confidence': round(random.uniform(0.7, 0.99), 3),
            'data_hash': f"hash_{i:06d}",
            'source': 'migration_sample'
        }
        
        sample_data.append(data)
    
    return sample_data

def filter_columns_for_db(df: pd.DataFrame) -> pd.DataFrame:
    """ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì— ë§ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§"""
    # ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì˜ ìœ íš¨í•œ ì»¬ëŸ¼ë“¤
    valid_columns = {
        'time', 'id', 'line', 'mold_name', 'working', 'molten_temp',
        'facility_operation_cycletime', 'production_cycletime',
        'low_section_speed', 'high_section_speed', 'cast_pressure',
        'biscuit_thickness', 'upper_mold_temp1', 'upper_mold_temp2',
        'lower_mold_temp1', 'lower_mold_temp2', 'sleeve_temperature',
        'physical_strength', 'coolant_temperature', 'ems_operation_time',
        'mold_code', 'passorfail', 'prediction_confidence', 'data_hash', 'source'
    }
    
    # í˜„ì¬ DataFrameì˜ ì»¬ëŸ¼ë“¤
    current_columns = set(df.columns)
    
    # ìœ íš¨í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    columns_to_keep = current_columns.intersection(valid_columns)
    
    # ë¬´ì‹œë˜ëŠ” ì»¬ëŸ¼ë“¤ ì¶œë ¥
    ignored_columns = current_columns - valid_columns
    if ignored_columns:
        print(f"âš ï¸  ë¬´ì‹œë˜ëŠ” ì»¬ëŸ¼ë“¤: {list(ignored_columns)}")
    
    print(f"âœ… ì‚¬ìš©ë˜ëŠ” ì»¬ëŸ¼ë“¤: {list(columns_to_keep)}")
    
    # í•„í„°ë§ëœ DataFrame ë°˜í™˜
    filtered_df = df[list(columns_to_keep)].copy()
    
    # ê¸°ë³¸ê°’ ì¶”ê°€ (í•„ìš”í•œ ê²½ìš°)
    if 'source' not in filtered_df.columns:
        filtered_df['source'] = 'csv_import'
    
    return filtered_df

def insert_data_batch(data_list: List[Dict]) -> Dict[str, any]:
    """ë°°ì¹˜ë¡œ ë°ì´í„° ì‚½ì…"""
    engine = get_db_engine()
    if not engine:
        return {'success': False, 'message': 'ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨', 'inserted_count': 0}
    
    try:
        # DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(data_list)
        
        # time ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        if 'time' in df.columns:
            df['time'] = pd.to_datetime(df['time'])
        
        # ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì— ë§ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
        df = filter_columns_for_db(df)
        
        # None ê°’ ì²˜ë¦¬
        df = df.where(pd.notnull(df), None)
        
        print(f"ğŸ“Š ì‚½ì…í•  ë°ì´í„° shape: {df.shape}")
        print(f"ğŸ“‹ ìµœì¢… ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì‚½ì…
        rows_inserted = df.to_sql('sensor_data', engine, if_exists='append', 
                                 index=False, method='multi')
        
        logger.info(f"ë°°ì¹˜ ë°ì´í„° ì‚½ì… ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")
        
        return {
            'success': True,
            'message': f'ì„±ê³µì ìœ¼ë¡œ {len(df)}ê°œì˜ ë ˆì½”ë“œë¥¼ ì‚½ì…í–ˆìŠµë‹ˆë‹¤.',
            'inserted_count': len(df)
        }
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ë°ì´í„° ì‚½ì… ì‹¤íŒ¨: {e}")
        return {
            'success': False,
            'message': f'ë°ì´í„° ì‚½ì… ì¤‘ ì˜¤ë¥˜: {str(e)}',
            'inserted_count': 0
        }

def insert_from_json_file(file_path: str) -> Dict[str, any]:
    """JSON íŒŒì¼ì—ì„œ ë°ì´í„° ì½ì–´ì„œ ì‚½ì…"""
    try:
        file_path = Path(file_path)
        if not file_path.exists():
            return {'success': False, 'message': f'íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}', 'inserted_count': 0}
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
        if not isinstance(data, list):
            if isinstance(data, dict) and 'data' in data:
                data = data['data']  # ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
            else:
                return {'success': False, 'message': 'JSON íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.', 'inserted_count': 0}
        
        return insert_data_batch(data)
        
    except Exception as e:
        logger.error(f"JSON íŒŒì¼ì—ì„œ ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {e}")
        return {'success': False, 'message': f'JSON íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}', 'inserted_count': 0}

def insert_from_csv_file(file_path: str, limit: Optional[int] = None) -> Dict[str, any]:
    """CSV íŒŒì¼ì—ì„œ ë°ì´í„° ì½ì–´ì„œ ì‚½ì…"""
    try:
        # limitì´ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ ê°œìˆ˜ë§Œ ì½ê¸°
        if limit:
            df = pd.read_csv(file_path, nrows=limit)
            print(f"ğŸ“„ CSV íŒŒì¼ì—ì„œ ìƒìœ„ {limit}ê°œ í–‰ ì½ê¸°: {file_path}")
        else:
            df = pd.read_csv(file_path)
            print(f"ğŸ“„ CSV íŒŒì¼ ì „ì²´ ì½ê¸°: {file_path}")
        
        print(f"ğŸ“Š ì½ì€ ë°ì´í„° í–‰ ìˆ˜: {len(df)}")
        
        # ì»¬ëŸ¼ ì •ë³´ ì¶œë ¥
        print(f"ğŸ“‹ ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ (ìœ ì—°í•˜ê²Œ ì²˜ë¦¬)
        time_columns = [col for col in df.columns if 'time' in col.lower() or 'timestamp' in col.lower()]
        pass_fail_columns = [col for col in df.columns if 'pass' in col.lower() or 'fail' in col.lower()]
        
        if not time_columns:
            # time ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ìƒì„±
            df['time'] = pd.date_range(start=datetime.now(), periods=len(df), freq='5T')
            print("âš ï¸  ì‹œê°„ ì»¬ëŸ¼ì´ ì—†ì–´ì„œ ìë™ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        else:
            # ì²« ë²ˆì§¸ ì‹œê°„ ì»¬ëŸ¼ ì‚¬ìš©
            time_col = time_columns[0]
            if time_col != 'time':
                df['time'] = df[time_col]
                print(f"ğŸ“… ì‹œê°„ ì»¬ëŸ¼ ì‚¬ìš©: {time_col}")
        
        if not pass_fail_columns:
            # passorfail ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ëœë¤ ìƒì„±
            df['passorfail'] = ['Pass' if random.random() > 0.1 else 'Fail' for _ in range(len(df))]
            print("âš ï¸  Pass/Fail ì»¬ëŸ¼ì´ ì—†ì–´ì„œ ëœë¤ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        else:
            # ì²« ë²ˆì§¸ pass/fail ì»¬ëŸ¼ ì‚¬ìš©
            pass_fail_col = pass_fail_columns[0]
            if pass_fail_col != 'passorfail':
                df['passorfail'] = df[pass_fail_col]
                print(f"âœ… Pass/Fail ì»¬ëŸ¼ ì‚¬ìš©: {pass_fail_col}")
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        print("\nğŸ“‹ ì½ì€ ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 3í–‰):")
        print(df.head(3).to_string())
        
        # DataFrameì„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        data_list = df.to_dict('records')
        
        return insert_data_batch(data_list)
        
    except Exception as e:
        logger.error(f"CSV íŒŒì¼ì—ì„œ ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {e}")
        return {'success': False, 'message': f'CSV íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}', 'inserted_count': 0}

def insert_top_n_from_csv(file_path: str, n: int = 10) -> Dict[str, any]:
    """CSV íŒŒì¼ì—ì„œ ìƒìœ„ Nê°œ í–‰ë§Œ ì½ì–´ì„œ ì‚½ì…"""
    return insert_from_csv_file(file_path, limit=n)

def get_current_data_count() -> int:
    """í˜„ì¬ ì €ì¥ëœ ë°ì´í„° ê°œìˆ˜ ì¡°íšŒ"""
    engine = get_db_engine()
    if not engine:
        return -1
    
    try:
        with engine.connect() as conn:
            result = conn.execute(text("SELECT COUNT(*) FROM sensor_data"))
            count = result.scalar()
            return count
            
    except Exception as e:
        logger.error(f"ë°ì´í„° ê°œìˆ˜ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        return -1

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(description='TimescaleDB ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬')
    parser.add_argument('--init', action='store_true', help='í…Œì´ë¸” ì´ˆê¸°í™”')
    parser.add_argument('--sample', type=int, metavar='COUNT', help='ìƒ˜í”Œ ë°ì´í„° ìƒì„± ë° ì‚½ì…')
    parser.add_argument('--json', type=str, metavar='FILE', help='JSON íŒŒì¼ì—ì„œ ë°ì´í„° ì‚½ì…')
    parser.add_argument('--csv', type=str, metavar='FILE', help='CSV íŒŒì¼ì—ì„œ ë°ì´í„° ì‚½ì…')
    parser.add_argument('--csv-top', type=str, metavar='FILE', help='CSV íŒŒì¼ì—ì„œ ìƒìœ„ 10ê°œë§Œ ì‚½ì…')
    parser.add_argument('--limit', type=int, default=10, metavar='N', help='CSVì—ì„œ ì½ì„ í–‰ ìˆ˜ (ê¸°ë³¸ê°’: 10)')
    parser.add_argument('--count', action='store_true', help='í˜„ì¬ ë°ì´í„° ê°œìˆ˜ í™•ì¸')
    
    args = parser.parse_args()
    
    print("=== TimescaleDB ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬ ===")
    
    # í˜„ì¬ ë°ì´í„° ê°œìˆ˜ í™•ì¸
    if args.count or len(sys.argv) == 1:
        current_count = get_current_data_count()
        if current_count >= 0:
            print(f"ğŸ“Š í˜„ì¬ ì €ì¥ëœ ë°ì´í„°: {current_count:,}ê°œ")
        else:
            print("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return
    
    # í…Œì´ë¸” ì´ˆê¸°í™”
    if args.init:
        print("ğŸ”§ í…Œì´ë¸” ì´ˆê¸°í™” ì¤‘...")
        if init_sensor_data_table():
            print("âœ… í…Œì´ë¸” ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            print("âŒ í…Œì´ë¸” ì´ˆê¸°í™” ì‹¤íŒ¨")
            return
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    if args.sample:
        print(f"ğŸ² ìƒ˜í”Œ ë°ì´í„° {args.sample}ê°œ ìƒì„± ì¤‘...")
        sample_data = generate_sample_data(args.sample)
        result = insert_data_batch(sample_data)
        
        if result['success']:
            print(f"âœ… {result['message']}")
        else:
            print(f"âŒ {result['message']}")
    
    # JSON íŒŒì¼ì—ì„œ ì‚½ì…
    if args.json:
        print(f"ğŸ“„ JSON íŒŒì¼ì—ì„œ ë°ì´í„° ì‚½ì… ì¤‘: {args.json}")
        result = insert_from_json_file(args.json)
        
        if result['success']:
            print(f"âœ… {result['message']}")
        else:
            print(f"âŒ {result['message']}")
    
    # CSV íŒŒì¼ì—ì„œ ì „ì²´ ì‚½ì…
    if args.csv:
        print(f"ğŸ“„ CSV íŒŒì¼ì—ì„œ ë°ì´í„° ì‚½ì… ì¤‘: {args.csv}")
        result = insert_from_csv_file(args.csv)
        
        if result['success']:
            print(f"âœ… {result['message']}")
        else:
            print(f"âŒ {result['message']}")
    
    # CSV íŒŒì¼ì—ì„œ ìƒìœ„ Nê°œë§Œ ì‚½ì…
    if args.csv_top:
        print(f"ğŸ“„ CSV íŒŒì¼ì—ì„œ ìƒìœ„ {args.limit}ê°œ ë°ì´í„° ì‚½ì… ì¤‘: {args.csv_top}")
        result = insert_top_n_from_csv(args.csv_top, args.limit)
        
        if result['success']:
            print(f"âœ… {result['message']}")
        else:
            print(f"âŒ {result['message']}")
    
    # ìµœì¢… ë°ì´í„° ê°œìˆ˜ í™•ì¸
    if args.init or args.sample or args.json or args.csv or args.csv_top:
        final_count = get_current_data_count()
        if final_count >= 0:
            print(f"ğŸ“Š ìµœì¢… ë°ì´í„° ê°œìˆ˜: {final_count:,}ê°œ")

if __name__ == "__main__":
    main()